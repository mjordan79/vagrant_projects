# -*- mode: ruby -*-
# vi: set ft=ruby :

# Creates an environment with a variable number of nodes, each visible in the private subnetwork.
# Node 1 is supposed to be the master, the others are the workers.
# All the nodes are based on Ubuntu 22.04 LTS and the Hyper-V hypervisor.
# Requires the vagrant-hostmanager plugin:
# 	Install globally with: vagrant plugin install vagrant-hostmanager beforehand.

# For a static IP on Hyper-V, we also need to create an ad-hoc hyper-v virtual switch.
# Please read the Powershell script in ./scripts/create-nat-hyperv-switch.ps1
# Requires the vagrant-reload plugin:
#   Install globally with: vagrant plugin install vagrant-reload
# Otherwise it will be installed locally each time you create the environments.
# 
# IMPORTANT: When asked, choose the Default Switch as the network adapter. It will be switched automatically
# to LabSwitch after the first boot.

# Maintainer: Renato Perini <renato.perini@gmail.com>

Vagrant.require_version ">= 2.3.7"

# Provisioning stuff we don't want to put in other script files.
$script = <<-SCRIPT
#!/usr/bin/env bash

echo "Updating neofetch ..."
curl -s https://raw.githubusercontent.com/dylanaraps/neofetch/master/neofetch -o /usr/bin/neofetch
chmod +x /usr/bin/neofetch

grep -qxF 'neofetch --ascii_distro Ubuntu' $HOME/.bash_profile || echo 'neofetch --ascii_distro Ubuntu' >> $HOME/.bash_profile
grep -qxF 'neofetch --ascii_distro Ubuntu' /home/vagrant/.bash_profile || echo 'neofetch --ascii_distro Ubuntu' >> /home/vagrant/.bash_profile
systemctl disable --now ufw
grep -qxF 'alias ls="ls --color"' $HOME/.bashrc || echo 'neofetch --ascii_distro Ubuntu' >> $HOME/.bashrc
grep -qxF 'alias ls="ls --color"' /home/vagrant/.bashrc || echo 'neofetch --ascii_distro Ubuntu' >> /home/vagrant/.bashrc
grep -qxF 'source ./.bashrc' $HOME/.bash_profile || echo 'source ./.bashrc' >> $HOME/.bash_profile
echo "Disabling SELinux ..."
setenforce 0
sed -i --follow-symlinks 's/SELINUX=permissive/SELINUX=disabled/g' /etc/selinux/config
sed -i 's/^DNS=.*/DNS=8.8.8.8/g' /etc/systemd/resolved.conf
sed -i 's/^FallbackDNS=.*/FallbackDNS=8.8.4.4/g' /etc/systemd/resolved.conf

SCRIPT

Vagrant.configure(2) do |config| 
	
	# Config parameters for the environments
	number_of_nodes = 6
	cpus_per_node = 32
	mem_per_node = 16384
	machine_name_prefix = "node"
	regular_user = "user"
	regular_password = "user"
	root_password = "vagrant"
	update_distro = true
	enable_zsh = false
	enable_docker = false
	# enable_kubernetes = true subdivides the total number of nodes in 3 control plane nodes and 1 load balancer node. The remaining nodes
	# are worker nodes. So if you choose, for example, number_of_nodes = 6, it will create 1 load balancer with 1GB of RAM,
	# 3 control plane nodes and 2 worker nodes. Make sure number_of_nodes in this case is always bigger than 4. Memory and CPUs for 
	# these spacial nodes are fixed: 1024M / 3 vCPUs for the load balancer, 2048M / 4 vCPUs for the control plane nodes. The worker nodes
	# are associated with mem_per_node amount and cpus_per_node. The load balancer node will install (and configure) an HAProxy instance 
	# while it will be installed Kubernetes through MicroK8s / snapd on the other nodes. 
	# If enable_kubernetes = false, the nodes will be created all equal following the specs given with the cpus_per_node and mem_per_node parameters.
	enable_kubernetes = true
	kubernetes_version = 1.28
	enable_java = false
	enable_gitea = false
	enable_nfs = true
	enable_synced_folder = false
	smb_user = ""
	smb_pass = ""
	
	# Require some plugins.
	config.vagrant.plugins = ["vagrant-hostmanager","vagrant-reload"]

	# Enable /etc/hosts handling automatically.
	if Vagrant.has_plugin?("vagrant-hostmanager")
		config.hostmanager.enabled = false
		config.hostmanager.manage_host = false
		config.hostmanager.manage_guest = true
		config.hostmanager.ignore_private_ip = true
		config.hostmanager.include_offline = true
    end

	# Define a start trigger.
	config.trigger.before :up do |trigger|
		trigger.info = "Creating 'LabSwitch' Hyper-V switch if it does not exist..."
		trigger.run = {
			privileged: "true", powershell_elevated_interactive: "true", path: "./scripts/powershell/create-nat-hyperv-switch.ps1"
		}
	end

	# Provision the nodes.
	(1..number_of_nodes).each do |i|
        config.vm.define "#{machine_name_prefix}-#{i}", primary: true do |node|
	    	node.vm.box = "generic/ubuntu2204"
			if enable_kubernetes == true && i == 1
				node.vm.hostname = "#{machine_name_prefix}-lb"
			elsif enable_kubernetes == true && i == 2 || i == 3 || i == 4 
				node.vm.hostname = "#{machine_name_prefix}-master#{i - 1}"
			elsif enable_kubernetes == true && i > 4
	    	    node.vm.hostname = "#{machine_name_prefix}-worker#{i - 4}"
			else
				node.vm.hostname = "#{machine_name_prefix}-#{i}"
			end
	    	node.vm.provider "hyperv" do |hyperv|
	    		hyperv.enable_virtualization_extensions = true
        		hyperv.linked_clone = true
				if enable_kubernetes == true && i == 1
					hyperv.cpus = 3
					hyperv.memory = 1024
				elsif enable_kubernetes == true &&  i == 2 || i == 3 || i == 4 
					hyperv.cpus = 4
					hyperv.memory = 2048
				elsif enable_kubernetes = true && i > 4
					hyperv.cpus = cpus_per_node
					hyperv.memory = mem_per_node
				else 
					hyperv.cpus = cpus_per_node
					hyperv.memory = mem_per_node
				end
				hyperv.vmname = "#{machine_name_prefix}-#{i}"
				hyperv.maxmemory = nil
	    	end
                    
	    	# Define a reload trigger.
	    	node.trigger.before :reload do |trigger|
	    		trigger.info = "Setting Hyper-V switch to 'LabSwitch' to allow for static IP..."
	    		trigger.run = {
	    			privileged: "true", powershell_elevated_interactive: "true", path: "./scripts/powershell/set-hyperv-switch.ps1", args: "#{machine_name_prefix}-#{i}"
	    		}
	    	end
    
			# Update the distro
			if update_distro == true then
	    		node.vm.provision "shell", path: "./scripts/bash/update_distro.sh"
			end
	    	# Install basic software.
			node.vm.provision "shell", path: "./scripts/bash/basic_software_provision.sh"
			# Install Docker
			if enable_docker == true then
	    		node.vm.provision "shell", path: "./scripts/bash/docker_provision.sh"
			end
			# Install ZSH, Oh-My-ZSH, Powerline10k, Dracula Theme for tmux
			if enable_zsh == true then
				node.vm.provision "shell", path: "./scripts/bash/zsh_provision.sh"
			end
            # Mixed actions, mostly unrelated to each other.
	    	node.vm.provision "shell", inline: $script
			# We want a normal user. With Bash or ZSH if available.
	    	node.vm.provision "shell", path: "./scripts/bash/add_sudo_user.sh" do |s|
	    		s.args = ["#{regular_user}", "#{regular_password}", "#{root_password}"]
	    	end
			# And we have to work with static IPs. We don't need IPV6.
	    	node.vm.provision "shell", path: "./scripts/bash/configure_static_ip.sh" do |s|
	        	s.args = "#{i}"
	    	end

			# Write the IPS into the /etc/hosts file. Still not the static IP but we need the entry to add the localhost alias in the next sed command.
			node.vm.provision :hostmanager, run: 'always'

			# Include the alias 127.0.0.1 to localhost in /etc/hosts file.
			node.vm.provision :shell, run: 'always', privileged: 'true', inline: "sed -i '1s/.*/127.0.0.1 localhost/' /etc/hosts"
				    			
	    	# Reload the machine.
	    	node.vm.provision :reload
            
			# Provision HAProxy in the load balancer node if enable_kubernetes is enabled.
            if enable_kubernetes == true && i == 1
                node.vm.provision "shell", path: "./scripts/bash/haproxy_provision.sh" do |s|
					# If enable_kubernetes is set and we're on the first node, provision HAProxy
					s.args = ["#{i}", "#{enable_kubernetes}"]
				end
			end

			# Provision Kubernetes through MicroK8S. Provisioning occurs from node 2 onwards.
            if enable_kubernetes == true && i >= 2
                node.vm.provision "shell", path: "./scripts/bash/kubernetes_provision.sh" do |s|
					s.args = ["#{i}", "#{kubernetes_version}"]
				end
			end

			# Enable Java Development Kit (JDK 17 from the Eclipse Temurin Project)
			if enable_java == true then
                node.vm.provision "shell", path: "./scripts/bash/java_provision.sh"
			end

			# Provision NFS Server / Clients
			if enable_nfs == true then
                node.vm.provision "shell", path: "./scripts/bash/nfs_provision.sh" do |s|
					# If enable_kubernetes == true, we enable the NFS server only starting from first control plane node.
					s.args = ["#{i}", "#{enable_kubernetes}"]
				end
			end

			# Just configure the IPs
			node.vm.provision :hostmanager, run: 'always'
			
			# Provision the Gitea Git Server
			if enable_gitea == true then
				node.vm.provision "shell", path: "./scripts/bash/gitea_provision.sh" do |s|
					s.args = "#{i}"
				end
			end
	    	
			# Enable synced folder. Be sure to fill in the administrative username and password of your Windows account.
			if enable_synced_folder == true then
	    		node.vm.synced_folder "./#{machine_name_prefix}_#{i}", "/vagrant", create: true, type: "smb",
	    			smb_password: "#{smb_pass}", smb_username: "#{smb_user}"
	    	else
				# Or disable synced folder
				node.vm.synced_folder ".", "/vagrant", disabled: true	
			end
	    end		
	end
end
